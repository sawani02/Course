{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Classification\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Paul Rad, Ph.D.** </span>  \n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Gonzalo De La Torre, Ph.D. Student** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Open Cloud Institute, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> gonzalo.delatorreparra@utsa.edu, paul.rad@utsa.edu </span>  \n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Classification using Machine Learning\n",
    "\n",
    "Email classification is a common beginner problem from Natural Language Processing (NLP). The idea is simple - given an email you’ve never seen before, determine whether or not that email is Spam or not (aka Ham).\n",
    "\n",
    "While the classification between spam and non-spam email task is easy for humans, it’s much harder to write a program that can correctly classify an email as Spam or Ham. In the following program, instead of telling the program which words we think are important, we will proceed to let the program learn which words are actually important.\n",
    "\n",
    "To tackle this problem, we start with a collection of sample emails (i.e. a text corpus). In this corpus, each email has already been labeled as Spam or Ham. Since we are making use of these labels in the training phase, this is a supervised learning task. This is called supervised learning because we are (in a sense) supervising the program as it learns what Spam emails look like and what Ham email look like.\n",
    "\n",
    "During the training phase, we present these emails and their labels to the program. For each email, the program says whether it thought the email was Spam or Ham. After the program makes a prediction, we tell the program what the label of the email actually was. The program then changes its configuration so as to make a better prediction the next time around. This process is done iteratively until either the program can’t do any better or we get impatient and just tell the program to stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Steps\n",
    "\n",
    "In this section we will start by importing the necessary libraries into our machine learning program. One of the main libraries we are importing is **tensorflow** which is the library we will be using to perform many of our deep learning computations. In addition, we will also import the pre-labeled email data contained in the **data.tar.gz** file and set variables where the number of words within an email will be saved **numFeatures** and the and classification (ham or spam) is stated **numLabels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Display plots inline \n",
    "%matplotlib inline\n",
    "\n",
    "# import email data\n",
    "\n",
    "def csv_to_numpy_array(filePath, delimiter):\n",
    "    return np.genfromtxt(filePath, delimiter=delimiter, dtype=None)\n",
    "\n",
    "def import_data():\n",
    "    if \"data\" not in os.listdir(os.getcwd()):\n",
    "        # Untar directory of data if we haven't already\n",
    "        tarObject = tarfile.open(\"/home/gonzalo/tensorflow-tutorial/data.tar.gz\")\n",
    "        tarObject.extractall()\n",
    "        tarObject.close()\n",
    "        print(\"Extracted tar to current directory\")\n",
    "    else:\n",
    "        # we've already extracted the files\n",
    "        pass\n",
    "\n",
    "    print(\"loading training data\")\n",
    "    trainX = csv_to_numpy_array(\"data/trainX.csv\", delimiter=\"\\t\")\n",
    "    trainY = csv_to_numpy_array(\"data/trainY.csv\", delimiter=\"\\t\")\n",
    "    print(\"loading test data\")\n",
    "    testX = csv_to_numpy_array(\"data/testX.csv\", delimiter=\"\\t\")\n",
    "    testY = csv_to_numpy_array(\"data/testY.csv\", delimiter=\"\\t\")\n",
    "    return trainX,trainY,testX,testY\n",
    "\n",
    "trainX,trainY,testX,testY = import_data()\n",
    "\n",
    "# set parameters for training\n",
    "\n",
    "# features, labels\n",
    "numFeatures = trainX.shape[1]\n",
    "numLabels = trainY.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Defining your placeholders\n",
    "\n",
    "A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define placeholders and variables for use in training\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=(np.sqrt(6/numFeatures+\n",
    "                                                         numLabels+1)),\n",
    "                                       name=\"weights\"))\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=(np.sqrt(6/numFeatures+numLabels+1)),\n",
    "                                    name=\"bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Variables\n",
    "\n",
    "After definining our **placeholders** we must proceed to initialize all the variables and define additional functions using tensorflow library to compute define a feedforward algorithm, cost function, optimization algorithms, and estimate our accuracy. At this point none of these operations would be executed, just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "init_OP = tf.initialize_all_variables()\n",
    "\n",
    "# define feedforward algorithm\n",
    "y = tf.nn.sigmoid(tf.add(tf.matmul(X, weights, name=\"apply_weights\"), bias, name=\"add_bias\"), name=\"activation\")\n",
    "\n",
    "# define cost function and optimization algorithm (gradient descent)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "cost_OP = tf.nn.l2_loss(y-yGold, name=\"squared_error_cost\")\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)\n",
    "\n",
    "# accuracy function\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(yGold,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Starting your Session\n",
    "\n",
    "Now that we have defined all of your elements, we can proceed to create and start the session that will execute all operations previous declarations. In this seccion we will first proceed to train our model with data extracted from the **data.tar.gz**, then test our output model by feeding  it with test data, and finally calculating the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numEpochs = 10000\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "# Launch the graph\n",
    "errors = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_OP )\n",
    "    print('Initialized Session.')\n",
    "    for step in range(numEpochs):\n",
    "        # run optimizer at each step in training\n",
    "        sess.run(training_OP, feed_dict={X: trainX, yGold: trainY})\n",
    "        # fill errors array with updated error values\n",
    "        accuracy_value = accuracy.eval(feed_dict={X: trainX, yGold: trainY})\n",
    "        errors.append(1 - accuracy_value)\n",
    "    print('Optimization Finished!')\n",
    "    \n",
    "    # output final error\n",
    "    print(\"Final error found during training: \", errors[-1])\n",
    "    # output accuracy \n",
    "    print(\"Final accuracy on test set: %s\" %str(sess.run(accuracy, \n",
    "                                                     feed_dict={X: testX, \n",
    "                                                                yGold: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot errors array to see how it decreased\n",
    "plt.plot([np.mean(errors[i-50:i]) for i in range(len(errors))])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
