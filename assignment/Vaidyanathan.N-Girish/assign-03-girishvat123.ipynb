{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2  Target\n",
      "0  2.067788  0.258133       1\n",
      "1  0.993994 -0.609145       1\n",
      "2 -0.690315  0.749921       0\n",
      "3  1.023582  0.529003       0\n",
      "4  0.700747 -0.496724       1\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "[[ 2.23285508  0.98599917]\n",
      " [-1.26625991 -0.54991925]\n",
      " [-0.68401438  0.45249093]\n",
      " [-1.10392606  1.56885862]]\n",
      "Error:  0.240000009537\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random\n",
    "import string\n",
    "\n",
    "learningrate=0.01\n",
    "iterations=2000\n",
    "num_inputs=2\n",
    "num_hidden=4\n",
    "num_output=2\n",
    "#import the data from the csv file\n",
    "tran = pd.read_csv(\"intro_to_ann.csv\")\n",
    "print (tran.head())\n",
    "X_data, Y_data = np.array(train.ix[:,0:2]), np.array(train.ix[:,2])\n",
    "#split into training and test data labels\n",
    "#reshape the array into one hot vector\n",
    "Y_reshape = (np.arange(2) == Y_data[:, None]).astype(np.float32)\n",
    "#keep placeholder values for x and y\n",
    "X = tf.placeholder(\"float\", shape=[None, 2])\n",
    "Y = tf.placeholder(\"float\", shape=[None,2])\n",
    "#Assign weights as variables since these values keep changing\n",
    "W1 = tf.Variable(tf.random_normal([num_inputs, num_hidden]))\n",
    "b1 = tf.Variable(tf.zeros([4]))\n",
    "y1 = tf.matmul(X, W1) + b1\n",
    "W2 = tf.Variable(tf.random_normal([num_hidden, num_output]))\n",
    "b2 = tf.Variable(tf.zeros([num_output]))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "#loss function is a cross entropy function which tries to reduce the losss\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y* tf.log(y2), reduction_indices=[1]))\n",
    "#We use a GradientDescent Optimizer which uses learning rate of 0.01 \n",
    "optimizer = tf.train.GradientDescentOptimizer(learningrate).minimize(cross_entropy)\n",
    "#It initializes all the variables\n",
    "init = tf.initialize_all_variables()\n",
    "#This is where actual computation of graph happens\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #compare the actual value with the predicted value\n",
    "    correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        sess.run(optimizer, feed_dict={X: X_data, Y: Y_reshape})\n",
    "        accuracy_value = sess.run(accuracy, feed_dict={X: X_data, Y: Y_reshape})\n",
    "        errors.append(1 - accuracy_value)\n",
    "    #We append the errors in an array and print the last minimized error\n",
    "    print(sess.run(W2))\n",
    "    print(\"Error: \", errors[-1])\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
